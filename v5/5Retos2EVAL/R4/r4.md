**Misión: Implementación de una Solución de IA Privada para Protección de Datos**

### Contexto
La empresa *Terraformadores de Venus* ha recibido una solicitud de una organización altamente preocupada por la seguridad de sus datos. Esta empresa, **CerebroTech**, desarrolla software financiero de alta sensibilidad y desea implementar una **solución de IA privada** dentro de su infraestructura, evitando cualquier dependencia de servicios en la nube que puedan comprometer la confidencialidad de sus datos.

Nuestra tarea consiste en proporcionarles una solución que permita desplegar un **modelo de lenguaje de gran escala (LLM) en local**, garantizando que su información no salga de sus **servidor de dedicado en AWS**. Para ello, utilizaremos **DeepSeek R1** junto con **Open WebUI**, empleando **Docker Compose** para facilitar la gestión de los servicios.

---

### Objetivos
1. **Desplegar un entorno de IA en local** utilizando Docker y Docker Compose.
2. **Configurar Open WebUI** como interfaz gráfica para interactuar con el modelo de IA.
3. **Ejecutar y administrar DeepSeek R1** dentro de un contenedor mediante Ollama.
4. **Asegurar la correcta ejecución y accesibilidad** de la solución.

---

### Requisitos Técnicos
Para llevar a cabo esta implementación, utilizaremos las siguientes tecnologías:
- **Docker**: Para contenerizar la aplicación y sus dependencias.
- **Docker Compose**: Para administrar múltiples servicios en conjunto.
- **Ollama**: Para ejecutar modelos de lenguaje en local.
- **Open WebUI**: Para proporcionar una interfaz gráfica de uso amigable.
- **DeepSeek R1**: Modelo de lenguaje a implementar.

---

### Pasos de Implementación

1. **Configurar el archivo `docker-compose.yml`** con los servicios necesarios:
   - Un servicio para **Open WebUI**.
   - Un servicio para **Ollama** que ejecutará DeepSeek R1.

2. **Desplegar los contenedores** con el siguiente comando:
   ```sh
   docker compose up -d
   ```

3. **Descargar e instalar el modelo DeepSeek R1** en el contenedor de Ollama:
   ```sh
   docker exec -it ollama-ollama ollama pull deepseek-r1
   ```

4. **Acceder a la interfaz gráfica de Open WebUI** en `http://localhost:3000` para interactuar con el modelo.

5. **Validar el funcionamiento en línea de comandos** ejecutando:
   ```sh
   docker exec -it ollama-ollama ollama run deepseek-r1
   ```

---

### Consideraciones Adicionales
- Se recomienda probar diferentes modelos de la biblioteca de **Ollama** para evaluar cuál se adapta mejor a las necesidades de *CerebroTech*.
- Es importante verificar el **uso de recursos** del servidor donde se ejecutará la solución.
- Como parte del despliegue, se puede explorar la integración de **autenticación** en Open WebUI para restringir el acceso a usuarios autorizados.

---


